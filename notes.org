* Chapter 1
** Beta normal form is when you cannot beta reduce (apply lambdas to arguments) the terms any further. 
** Lambda calculus is a formal system for expressing programs in terms of abstraction and application.
* Chapter 2
   * /Reducing/ an expression means evaluating the terms until you’re left with a value
   * Expression is in canonical or /normal form/ when it reaches the number 6 because the value 6 has no remaining reducible expressions. 
Haskell’s nonstrict evaluation means not everything will get reduced to its irreducible form immediately, so this:
 ~(\f -> (1, 2 + f)) 2~
reduces to the following in WHNF:
 ~(1, 2 + 2)~
   * /let/ introduces an expression, so it can be used wherever you can have an expression, but /where/ is a declaration and is bound to a surrounding syntactic construct.
   *  In Haskell, an /expression/ is a well-structured combination of constants, variables, and functions.
* Chapter 4
    * /Data declarations/ are how datatypes are defined.
    * /The type constructor/ is the name of the type and is capitalized.
    * Data constructors are the values that inhabit the type they are defined in.
    * Example of data desclaration:
   #+BEGIN_SRC haskell
   data Bool = False | True
   --   [1]     [2] [3] [4]
   #+END_SRC 
   1. Type constructor for datatype Bool. This is the name of the type and shows up in type signatures.
   2. Data constructor for the value False.
   3. Pipe | indicates a sum type or logical disjunction: “or.” So, a Bool value is True or False.
   4. Data constructor for the value True.
** Numeric types:
   1. Integral Numbers:
      1. Int
      2. Integer
   2. Fractional
      1. Float
      2. Double
      3. Rational
** Typeclass
   * /Typeclass/ is a set of operations defined with respect to a polymorphic type.
   When a type is an instance of a typeclass, values of that type can be used in the standard operations defined for that typeclass. In Haskell, typeclasses are unique pairings of class and concrete instance. This means that if a given type a has an instance of Eq, it has only one instance of Eq.
* Chapter 5
    * In Haskell, you cannot create untyped data, so except for a sprinkling of syntactic sugar for things like numbers or functions, everything originates in a data constructor from some definition of a type.
    * The arrow, ~(->)~, is the *type constructor* for functions in Haskell.
#+BEGIN_SRC haskell
(Ord a, Num a) => a -> a -> Ordering
#+END_SRC
    Here, the constraints look like a tuple but they don’t add another function argument that you must provide, and they don’t appear as a tuple at the value or term level. Nothing to the left of the type-class arrow, =>, shows up at term level. The tuple of constraints does represent a product, or conjunction, of constraints.
    Compare:
   #+BEGIN_SRC haskell
       Prelude> :info (->)
       data (->) a b

     -- If you compare this to the type constructor for the two-tuple, you
     -- see the similarity:
       Prelude> :info (,)
       data (,) a b = (,) a b
   #+END_SRC
   Unlike the tuple constructor, though, the function type has no data constructors. The value that shows up at term level is the function. 
   * /Functions are values./
   The parameterization suggests that we will apply the function to some argument that will be bound to the first parameter, with the second parameter, b, representing the return or result type.
** Type signature constraints
   * Example - ~(Ord a, Num a) => a -> a -> Ordering~
   Here, the constraints look like a tuple but they don’t add another function argument that you must provide, and they don’t appear as a tuple at the value or term level. Nothing to the left of the type-class arrow, =>, shows up at term level. The tuple of constraints does represent a product, or conjunction, of constraints.
    * The way the ~(->)~ type constructor for functions works means ~a -> a -> a~ represents successive function applications, each taking one argument and returning one result. The difference is that the function at the outermost layer is actually returning another function that accepts the next argument. This is called currying.
    * Explicit parenthesization, as when an input parameter is itself a function (such as in map, above), may be used to indicate order of evaluation, but the implicit associativity of the function type does not mean the inner or final set of parentheses, i.e., the result type, evaluates first. Application is evaluation; in other words, the only way to evaluate anything is by applying functions, and function applica- tion is left associative. So, the leftmost, or outermost, arguments will be evaluated first, assuming anything gets evaluated (since Haskell is nonstrict, you can’t assume that anything will be evaluated, but this will be more clear later).
Also - https://stackoverflow.com/questions/36143423/right-associativity-in-type-signatures-of-functions
** Partial Application
   * ~(2^)~ (left section) is equivalent to ~(^) 2~ , or more verbosely ~\x -> 2 ^ x~
   * ~(^2)~ (right section) is equivalent to ~flip (^) 2~ , or more verbosely ~\x -> x ^ 2~
   * More - https://wiki.haskell.org/Section_of_an_infix_operator
Partial application is common enough in Haskell that, over time, you’ll develop an intuition for it. The sectioning syntax exists to allow some freedom in which argument of a binary operator you apply the function to.
** Polymorphism
   * Type signatures may have three kinds of types: concrete, constrained polymorphic, or parametrically polymorphic.
   * /Constrained/ = Ad-hoc polymorphyc. Ad-hoc polymorphism in Haskell is implemented with typeclasses.
   * /Typeclass constraints/ limit the set of potential types (and, thus, potential values) while also passing along the common functions that can be used with those values.
   * /Parametricity/ means that the behavior of a function with respect to the types of its (parametrically polymorphic) arguments is uniform.
   Parametricity is the property we get from having parametric polymorphism.
** Type inference
   * Haskell’s type inference is built on an extended version of the /Damas-Hindley-Milner/ type system.

* Chapter 6
** Haskell and purity (Show typeclass section)
   * /Side effect/ -- a potentially observable result apart from the value the expression evaluates to.
   Haskell manages effects by separating effectful computations from pure computations in ways that preserve the predictability and safety of function evaluation. Importantly, effect-bearing computations themselves become more composable and easier to reason about. The benefits of explicit effects include the fact that it makes it relatively easy to reason about and predict the results of our functions.
   * ~main~ has IO () type because ~main` /only/ produces side effects.
   * /IO/ is the type for values whose evaluation bears the possibility of causing side effects, such as printing text, reading text input from the user, reading or writing files, or connecting to remote computers.
** Instances are dispatched by type
* Chapter 7
** Interesting point about lambda functions
   > We won’t go into a lot of detail about this yet, but named entities and anonymous entities evaluate a bit differently in Haskell, and that can be one reason to use an anonymous function in some cases.
* Chapter 8
** Y-combinator
   * We use a combinator – known as the /Y combinator/ or fixed-point combinator – to write recursive functions in the lambda calculus. Haskell has native recursion ability based on the same principle as the Y combinator.
   * Also - http://mvanier.livejournal.com/2897.html
** Bottom
   * ⊥ or bottom is a term used in Haskell to refer to computations that do not successfully result in a value. The two main varieties of bottom are computations that failed with an error or those that failed to terminate.
* Chapter 9
** List's sintactic sugar
   * When we talk about lists, we often talk about them in terms of /cons cells/ and /spines/. 
     ** The cons cells are the list datatype’s second data constructor, a : [a], the result of recursively prepending a value to “more list.” The cons cell is a conceptual space that values may inhabit.
     ** The spine is the connective structure that holds the cons cells together and in place. As we will soon see, this structure nests the cons cells rather than ordering them in a right-to-left row. Because different functions may treat the spine and the cons cells differently, it is important to understand this underlying structure.
** Spines and nonstrict evaluation
   * When we talk about data structures in Haskell, particularly lists, sequences, and trees, we talk about them having a /spine/. This is the connective structure that ties the collection of values together.
   The problem with the ~1 : (2 : (3 : []))~ representation we used earlier is that it makes it seem like the value 1 exists “before” the cons (:) cell that contains it, but actually, the cons cells contain the values. Because of this and the way nonstrict evaluation works, you can evaluate cons cells independently of what they contain. It is possible to evaluate just the spine of the list without evaluating individual values. It is also possible to evaluate only part of the spine of a list and not the rest of it.
   Evaluation of the list in this representation proceeds /down/ the spine. Constructing the list when that is necessary, however, proceeds /up/ the spine.
   Because Haskell’s evaluation is nonstrict, the list isn’t constructed until it’s consumed — indeed, nothing is evaluated until it must be. Until it’s consumed or you force strictness in some way, there are a series of placeholders as a blueprint of the list that can be constructed when it’s needed.
   #+BEGIN_SRC 
  :                :  <------|
 / \              / \        |
1   :            _   :  <----|  -----  This is the "spine"
   / \     -->      / \      |
  2   :            _   :  <--|
     / \              / \
     3 []             _ []
   #+END_SRC
   * A special command in GHCi called ~sprint~ to print variables and see what has been evaluated already, with the underscore representing expressions that haven’t been evaluated yet.
   * GHC Haskell has some opportunistic optimizations which introduce strictness to make code faster when it won’t change how your code evaluates.
   * Polymorphism means values like Num a => a are really waiting for a sort of argument which will make it concrete. Due to it sprint may behave confusing.
   * Example of list evaluation:
#+BEGIN_SRC 
Prelude> :sprint blah
blah = _
-- The blah = _ indicates that blah is totally unevaluated. Next we’ll take one value from blah and then evaluate it by forcing GHCi to print the expression:
Prelude> take 1 blah
"a"
Prelude> :sprint blah
blah = 'a' : _
#+END_SRC
   * Spines are evaluated independently of values
   * *WHNF* (‘Weak head normal form’) means the expression is only evaluated as far as is necessary to reach a data constructor.
   Weak head normal form (WHNF) is a larger set and contains both the possibility that the expression is:
     1. Fully evaluated (*normal form*)
     2. Has been evaluated to the point of arriving at a *data constructor* 
     3. *Lambda* awaiting an argument.

   * /Weak Head/ is an expression which is evaluated up to /at least/ the first data constructor.
   * /Normal form/ exceeds that by requiring that all sub-expressions be fully evaluated.
   * Functions that are spine strict can force complete evaluation of the spine of the list even if they don’t force evaluation of each value.
     * Pattern matching is strict by default, so pattern matching on cons cells can mean forcing spine strictness if your function doesn’t stop recursing the list. It can evaluate the spine only or the spine as well as the values that inhabit each cons cell, depending on context.
   * /Lazy in the spine, strict in the leaves/
   We can have lazily evaluated code (e.g., map) wrapped around a strict core (e.g., +). In fact, we can choose to apply laziness and strictness in how we evaluate the spine or the leaves independently.

** Defenitions
   * /Cons cell/ is a data constructor and a product of the types a and [a] as defined in the list datatype.
   Because it references the list type constructor itself in the second argument, it allows for nesting of multiple cons cells, possibly indefinitely with the use of recursive functions, for representing an indefinite number of values in series:
#+BEGIN_SRC 
data [] a = [] | a : [a]
                   ^ cons operator
-- Defining it ourselves
data List a = Nil | Cons a (List a)
-- Creating a list using our list type
Cons 1 (Cons 2 (Cons 3 Nil))
#+END_SRC
   Here (Cons 1 ...), (Cons 2 ...) and (Cons 3 Nil) are all individual cons cells in the list [1, 2, 3].
   * The spine is a way to refer to the structure that glues a collection of values together. In the list datatype it is formed by the recursive nesting of cons cells. The spine is, in essence, the structure of collection that isn’t the values contained therein. Often spine will be used in reference to lists, but it applies with tree data structures as well

* Chapter 10
** Evaluation of fold functions
   * Folding is two stage process, and include /traversal/ and /folding/.
    * Traversal is the stage in which the fold recurses over the spine. Traversing the rest of the spine /doesn’t occur/ unless the function asks for the results of having folded the rest of the list. 
    * Folding refers to the evaluation or reduction of the folding function applied to the values. Given this two-stage process and non-strict evaluation, if ~f~ doesn’t evaluate its second argument (rest of the fold), no more of ~t~he spine will be forced. One of the consequences of this is that foldr can avoid evaluating not just some or all of the values in the list, but some or all of the list’s spine as well! For this reason, foldr can be used with lists that are potentially infinite.
** Types of Fold functions
*** Foldr
    * Definition:
#+BEGIN_SRC haskell
foldr :: (a -> b -> b) -> b -> [a] -> b
foldr f z [] = z
foldr f z (x:xs) = f x (foldr f z xs)
#+END_SRC
    * Associates to right, so:
#+BEGIN_SRC haskell
foldr (+) 0 [1, 2, 3]
-- expands to
1 + (2 + (3 + 0))
#+END_SRC
    * Traverses from head to tail, reduces backward
    * Works well with infinite lists:
#+BEGIN_SRC 
Prelude> myAny even [1..] -- myAny is defined through foldr
True
#+END_SRC

*** Foldl
    * Definition:
#+BEGIN_SRC haskell
foldl :: (b -> a -> b) -> b -> [a] -> b
foldl f acc [] = acc
foldl f acc (x:xs) = foldl f (f acc x) xs
#+END_SRC
    *  Associates to left, so:
#+BEGIN_SRC haskell
foldl (+) 0 (1 : 2 : 3 : [])
-- expands to
((0 + 1) + 2) + 3
#+END_SRC
    * Traverses and reduces in the same direction - from head to tail.
    * Recursion of the spine is unconditional:
    ~foldl~ must evaluate its whole spine before it starts evaluating values in each cell, it accumulates a pile of unevaluated values as it traverses the spine.
    Use foldl' (foldl-prime) to avoid this behavior.
    #+BEGIN_SRC haskell
    Prelude> foldr const 0 ([1..5] ++ undefined)
    1
    Prelude> foldr (flip const) 0 ([1..5] ++ undefined)
    *** Exception: Prelude.undefined
    #+END_SRC
** Tail call and tail recursion
   * A /tail call/ is the final result of a function. Some examples of tail calls in Haskell functions:
#+BEGIN_SRC haskell
  f x y z = h (subFunction x y z)
  where subFunction x y z = g x y z
  -- the ``tail call'' is h (subFunction x y z)
  -- or more precisely, h
#+END_SRC
   * /Tail recursion/ is a function whose /tail calls/ are recursive invocations of /itself/. This is distinguished from functions that call other functions in their tail call.
     * The below is not tail recursive, calls h, not itself.
     #+BEGIN_SRC haskell
       f x y z = h (subFunction x y z)
       where subFunction x y z = g x y z    
     #+END_SRC
     * Still not tail recursive. f is invoked again but not in the tail-call of f, it’s an argument to the actual tail-call h.
     #+BEGIN_SRC haskell
       f x y z = h (f (x - 1) y z)
     #+END_SRC
     * This is tail recursive. f is calling itself directly with no intermediaries.
     #+BEGIN_SRC haskell
       f x y z = f (x - 1) y z
     #+END_SRC
     * Not tail recursive, we give up control to the combining function f before continuing through the list. foldr’s recursive calls will bounce between foldr and f.
     #+BEGIN_SRC haskell
       foldr f z [] = z
       foldr f z (x:xs) = f x (foldr f z xs)    
     #+END_SRC
     * Tail recursive. foldl invokes itself recursively. The combining function is only an argument to the recursive fold.
     #+BEGIN_SRC haskell
       foldl f z [] = z
       foldl f z (x:xs) = foldl f (f z x) xs
     #+END_SRC
** scanl and scanr
   * scanl definition:
#+BEGIN_SRC haskell
scanl            :: (a -> b -> a) -> a -> [b] -> [a]
scanl f q xs     =  q : (case xs of
                            []   -> []
                            x:xs -> scanl f (f q x) xs)
#+END_SRC
   * scanr definition:
#+BEGIN_SRC haskell
scanr             :: (a -> b -> b) -> b -> [a] -> [b]
scanr f q0 []     =  [q0]
scanr f q0 (x:xs) =  f x q : qs
                     where qs@(q:_) = scanr f q0 xs 
#+END_SRC
* Chapter 11
** Algebraic Datatypes
   * A type can be thought of as an enumeration of constructors that have zero or more arguments.
** Data and type constructors
   * Type constructor:
     * used only at the type level, in type signatures and typeclass declarations and instances.
     * are static and resolve at compile time
   * Data constructor: 
     * construct the values at term level, values you can interact with at runtime
   * Type and data constructors that take no arguments are constants.
     * Example - Bool datatype
   * Information about types does not persist through to runtime. Data are what we’re working with at runtime.

** Cardinality
   * Datatypes that only contain a unary constructor always have the same cardinality as the type they contain.
     * For cardinality this means unary constructors are the identity function.
** newtype
   * Can have only a single unary constructor, so...
     * The cardinality of a newtype is the same as that of the type it contains.
   * cannot be a product type, sum type, or contain nullary constructors
   * has no runtime overhead, as it reuses the representation of the type it contains
     * It can do this because it’s not allowed to be a record (product type) or tagged union (sum type)
** Sum type
   * Interesting link about flexible instances - https://ghc.haskell.org/trac/haskell-prime/wiki/FlexibleInstances
** Product type
   * Any data constructor with two or more type arguments is a product.

** Constructing and deconstructing values
   * There are essentially two things we can do with a value. 
     * We can generate or construct it or...
     * we can match on it and consume it.
** Records Syntax
   * Whenever we have a product that uses record accessors, keep it separate of any sum type that is wrapping it.
#+BEGIN_SRC haskell
          -- Wrong
  data Automobile = Null
                  | Car { make :: String , model :: String , year :: Integer }
                  deriving (Eq, Show)
-- because of error when accidently accessing field with Null data:
 Prelude> make Null
"*** Exception: No match in record selector make 
#+END_SRC
    * Instead split out the product into an independent type with its own type constructor
** Function type is exponential
    * Given a function a -> b, we can calculate the inhabitants with the formula b^a
** Higher-kinded datatypes
   * Type constructors are functions one level up, structuring things that cannot exist at runtime — it’s purely static and describes the structure of your types
   * Kinds are the types of type constructors, primarily encoding the number of arguments they take.
   * Getting comfortable with higher-kinded types is important as type arguments provide a generic way to express a “hole” to be filled by consumers of your datatype later.
   * /higher kinded type/ is not to be confused with higher kinded /polymorphism/!
* Chapter 12
** Lifted and unlifted types
   * kind * is the kind of all standard lifted types, while types that have the kind # are unlifted.
   A lifted type, which includes any datatype you could define yourself, is any that can be inhabited by bottom. Lifted types are represented by a pointer and include most of the datatypes we’ve seen and most that you’re likely to encounter and use. Unlifted types are any type which cannot be inhabited by bottom. Types of kind # are often native machine types and raw pointers.
   Newtypes are a special case in that they are kind *, but are unlifted because their representation is identical to that of the type they contain, so the newtype itself is not creating any new pointer beyond that of the type it contains. That fact means that the newtype itself cannot be inhabited by bottom, only the thing it contains can be, so newtypes are unlifted. The default kind of concrete, fully-applied datatypes in GHC is kind *.
* Chapter 13
** Cabal and Stack
   * Cabal, or Common Architecture for Building Applications and Libraries, is a package manager.
     * Exists primarily to describe a single package with a Cabal file that has the .cabal file extension
   * Stack is a cross-platform program for developing Haskell projects.
     * Stack is built on top of Cabal in some important senses
     * Helps you manage both projects made up of multiple packages as well as individual packages
     * Relies on an LTS snapshot of Haskell packages from Stackage
** Stack commands flow
   * Setup GHC with stack ~stack setup~
   * Setup dependencies with ~stack build~
   * Run executable with ~stack exec~
     * Exec can be found in path: ~.stack-work/dist/{...noise...}/hello~ within your project directory
     * Exec building properties described in *.cabal file like that:
#+BEGIN_SRC 
executable hello
  hs-source-dirs:      src
  main-is:             Main.hs
  default-language:    Haskell2010
  build-depends:       base >= 4.7 && < 5
#+END_SRC
* Chapter 14
** Test types
***   Unit testing
     Unit testing allows the programmer to check that each function is performing the task it is meant to do. You assert that when the code runs with a specified input, the result is equal to the result you want.
*** Spec testing
     Spec testing is a somewhat newer version of unit testing. Like unit testing, it tests specific functions independently and asks you to assert that, when given the declared input, the result of the operation will be equal to the desired result.
*** Properties testing
     Property tests test the formal properties of programs without requiring formal proofs by allowing you to express a truth-valued, universally quantified (that is, will apply to all cases) function — usually equality — which will then be checked against randomly generated inputs.
     Property testing is fantastic for ensuring that you’ve met the minimum requirements to satisfy laws, such as the laws of monads or basic associativity. It is not appropriate for all programs, though, as it is not useful for times when there are no assertable, truth-valued properties of the software.
     Comparing to unit testing — the testing of individual units of code — Property testing is done with the assertion of laws or properties. 
** QuickCheck is for properties testing
   QuickCheck relies on a typeclass called Arbitrary and a newtype called ~Gen~ for generating its random data. arbitrary is a value of type Gen:
   #+BEGIN_SRC haskell
Prelude> :t arbitrary
arbitrary :: Arbitrary a => Gen a
   #+END_SRC
   This is merely a way to set a default generator for a type. When you use the arbitrary value, you have to specify the type to dispatch the right typeclass instance
   Use ~sample~ and ~sample'~ from the Test.QuickCheck module in order to get random data.
   #+BEGIN_SRC haskell
     Prelude> sample (arbitrary :: Gen Int)
     0
     <...>
     -4
     Prelude> sample (arbitrary :: Gen Double)
     0.0
     <...>
     26.87468214215407
   #+END_SRC
   You can create your own generator:
#+BEGIN_SRC haskell
-- this will return only one's when sampling
trivialInt :: Gen Int
trivialInt = return 1
-- this will return values from list when sampling:
oneThroughThree :: Gen Int
oneThroughThree = elements [1, 2, 2, 2, 2, 3]
#+END_SRC
